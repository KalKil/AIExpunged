{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "loading configuration file generation_config.json from cache at aitextgen/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from aitextgen import aitextgen\n",
    "\n",
    "ai = aitextgen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAre you an AI\u001b[0m person? What do you think of these projects? Let us know in the comments below.\n",
      "==========\n",
      "\u001b[1mAre you an AI\u001b[0m and a non-AI client, or just a system that you'd like to build or a non-AI client?\n",
      "\n",
      "Gundam is a great example of a non-AI client. The main problem you have with Gundams is that you don't have any client. It's not a good idea to build a non-AI client, because the client can't tell you how to go about building a non-AI client.\n",
      "\n",
      "What do you do if you want to build a non-AI client?\n",
      "\n",
      "Let's say you're building a non-AI client. You want to build a non-AI client. You want to install some other non-AI client, and you want to build a non-AI client. We might say, \"Well, let's build a non-AI client, and we'll install some other non-AI client, and we'll install a non-AI client, and we'll install a non-AI client, and we'll install a non-AI client, and we'll install a non-AI client, and we'll install a non-AI client, and we'll install a non-AI client, and we'll install a non-AI client?\"\n",
      "\n",
      "It\n",
      "==========\n",
      "\u001b[1mAre you an AI\u001b[0m or a human?\"\n",
      "\n",
      "\"Yeah, I can do that,\" she said. \"You can do that. I can do that.\"\n",
      "\n",
      "\"But you're a human, right?\"\n",
      "\n",
      "\"It's a different kind of thing.\"\n",
      "\n",
      "\"Okay, but what about the machines that make that stuff?\"\n",
      "\n",
      "\"Yeah, but we're all human. I'm a robot.\"\n",
      "\n",
      "\"Well, what do you think?\" asked Alice.\n",
      "\n",
      "\"I think you're a very good person. You're very capable of taking on different forms. But, because I'm not a robot, you're not a human. I'm almost like you, a kind of human. You are like me and you're like me, a kind of human and you're like me, but you're not a human. And you are a robot, you know.\"\n",
      "\n",
      "\"So you're not a human?\" asked Alice.\n",
      "\n",
      "\"That's a good question. I'm not human. I'm like you, a kind of human. You are a kind of human.\"\n",
      "\n",
      "\"Then what happens if you're a human?\" asked Alice.\n",
      "\n",
      "\"It's an artificial intelligence program that says, 'I know\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt = \"Are you an AI\", n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_tokenizer('Shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 81630.67it/s]\n"
     ]
    }
   ],
   "source": [
    "config = GPT2ConfigCPU()\n",
    "\n",
    "ai = aitextgen(tokenizer_file='aitextgen.tokenizer.json',config=config)\n",
    "\n",
    "data = TokenDataset('Shakespeare.txt', tokenizer_file='aitextgen.tokenizer.json', block_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin already exists in /trained_model and will be overwritten!\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                               \n",
      "\u001b[A                                                                          \u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "Loss: 3.170 — Avg: 3.196:  22%|██▏       | 11000/50000 [11:31<40:53, 15.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      "\u001b[A                                                                          \u001b[1m5,000 steps reached: generating sample texts.\u001b[0m\n",
      "Loss: 3.170 — Avg: 3.196:  22%|██▏       | 11000/50000 [11:31<40:53, 15.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      "\u001b[A                                                                          ==========\n",
      "                                                                               \n",
      "\u001b[A                                                                          .\n",
      "And now I have my life, and I will not learn'd\n",
      "But thou waste in thy brother's son's face.\n",
      "\n",
      "GLOUCESTER:\n",
      "And, by my lord, I am no rement to-night.\n",
      "\n",
      "KING RICHARD II:\n",
      "Do,\n",
      "                                                                               \n",
      "\u001b[A                                                                          ==========\n",
      "Loss: 3.170 — Avg: 3.196:  22%|██▏       | 11000/50000 [11:32<40:53, 15.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=5000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.120 — Avg: 3.106: 100%|██████████| 5000/5000 [03:31<00:00, 23.61it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "ai.train(data, batch_size=8, num_steps=5000, generate_every=5000, save_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mROMEO:\u001b[0m\n",
      "O me, I'll nothing, behold.\n",
      "\n",
      "GONZEL:\n",
      "But I, no more.\n",
      "\n",
      "FLORIZEL:\n",
      "One that I would have none of speech\n",
      "Cry:\n",
      "Go, my lord, if\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "O, in my grace, and tell me,\n",
      "And dream, each one of the bodyard\n",
      "And I am strong to charg: but who he shall be\n",
      "That I can dance and to be a cry toped\n",
      "O\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Ah, that was the peace of the fine's eyes.\n",
      "\n",
      "POLIXENES:\n",
      "OXENES:\n",
      "Cally strewells, and turningly strengthen'd from him.\n",
      "\n",
      "TRANIO:\n",
      "\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "They have been\n",
      "A guier'd for his songs: and so it is,\n",
      "But that's a burial'st!\n",
      "That is my lady:\n",
      "O, she's not a merry, the thine, the first'st\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "For one words, or I'll make these remembrance,\n",
      "And yet thy constant'd books, within,\n",
      "A choice is a hurgh feast a curbble.\n",
      "\n",
      "SEBASTIAN:\n",
      "\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "O, hear me, go, then,\n",
      "Let them go.\n",
      "\n",
      "JULIET:\n",
      "Madam, good Petruchio!\n",
      "\n",
      "MIRANDA:\n",
      "Ay, I.\n",
      "\n",
      "GONZALO:\n",
      "He is the stern\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "This issue?\n",
      "\n",
      "POLIXENES:\n",
      "If this be done;\n",
      "The condition of this,\n",
      "One more cold-sisting: nay, and not thy heir,\n",
      "Do not dry'd with distress\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Come, you'll give me leave.\n",
      "\n",
      "BENVOLIO:\n",
      "I'll promise thee in this.\n",
      "\n",
      "PERDITA:\n",
      "I came, my lord,\n",
      "Whatsemble revengeance thee.\n",
      "\n",
      "POXENES\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "No, I pray thee; O thou shalt why,\n",
      "And doublet me with a sighnce a fault,\n",
      "That could all the duke in the morning.\n",
      "\n",
      "KING RICHARD III:\n",
      "May you this no tender th\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Nay, peace, and be it with the condition\n",
      "Of the people's slaining of the fatal grief,\n",
      "And in warlike, like a sorrow thine,\n",
      "Call to the cause that shriech,\n"
     ]
    }
   ],
   "source": [
    "ai.generate(10, prompt=\"ROMEO:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
